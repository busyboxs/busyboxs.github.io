<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>tensorflowå­¦ä¹ ç¬”è®°ï¼ˆ4ï¼‰ï¼šActivation Functions | Busyboxs</title><meta name="keywords" content="tensorflow"><meta name="author" content="Busyboxs"><meta name="copyright" content="Busyboxs"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Activation functions are a cornerstone of Machine Learning. In general, Activation Functions define how a processing unit will treat its input â€” usually passing this input through it and generating an">
<meta property="og:type" content="article">
<meta property="og:title" content="tensorflowå­¦ä¹ ç¬”è®°ï¼ˆ4ï¼‰ï¼šActivation Functions">
<meta property="og:url" content="https://busyboxs.github.io/blogs/2b14ee1d/index.html">
<meta property="og:site_name" content="Busyboxs">
<meta property="og:description" content="Activation functions are a cornerstone of Machine Learning. In general, Activation Functions define how a processing unit will treat its input â€” usually passing this input through it and generating an">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/covers/tensorflow.jpg">
<meta property="article:published_time" content="2017-08-27T13:00:00.000Z">
<meta property="article:modified_time" content="2019-08-13T02:43:49.627Z">
<meta property="article:author" content="Busyboxs">
<meta property="article:tag" content="tensorflow">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/covers/tensorflow.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://busyboxs.github.io/blogs/2b14ee1d/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google_site_verification" content="FwkuU16DW0uUtLvBlI63aK9-eCBhC55pVQMscORY34M"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?0737f86b9c26e7c4269a4dec38cea0c8";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=UA-158970947-1"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-158970947-1');
</script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lacquer&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: 'å¤åˆ¶æˆåŠŸ',
    error: 'å¤åˆ¶é”™è¯¯',
    noSupport: 'æµè§ˆå™¨ä¸æ”¯æŒ'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'å¤©',
  date_suffix: {
    just: 'åˆšåˆš',
    min: 'åˆ†é’Ÿå‰',
    hour: 'å°æ—¶å‰',
    day: 'å¤©å‰',
    month: 'ä¸ªæœˆå‰'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'tensorflowå­¦ä¹ ç¬”è®°ï¼ˆ4ï¼‰ï¼šActivation Functions',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2019-08-13 10:43:49'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">åŠ è½½ä¸­...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">90</div></a><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">13</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> é“¾æ¥</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> å…³äº</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/img/archive_img.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Busyboxs</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> é“¾æ¥</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> å…³äº</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">tensorflowå­¦ä¹ ç¬”è®°ï¼ˆ4ï¼‰ï¼šActivation Functions</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2017-08-27T13:00:00.000Z" title="å‘è¡¨äº 2017-08-27 21:00:00">2017-08-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2019-08-13T02:43:49.627Z" title="æ›´æ–°äº 2019-08-13 10:43:49">2019-08-13</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/tensorflow/">tensorflow</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">å­—æ•°æ€»è®¡:</span><span class="word-count">1.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»æ—¶é•¿:</span><span>7åˆ†é’Ÿ</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="tensorflowå­¦ä¹ ç¬”è®°ï¼ˆ4ï¼‰ï¼šActivation Functions"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»é‡:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>Activation functions are a cornerstone of Machine Learning. In general, Activation Functions define how a processing unit will treat its input â€” usually passing this input through it and generating an output through its result. To begin the process of having a more intuitive understanding, letâ€™s go through some of the most commonly used functions.</p>
<p><strong>Importing Dependencies</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<p>The next cell implements a basic function that plots a surface for an arbitrary activation function. The plot is done for all possible values of weight and bias between -0.5 and 0.5 with a step of 0.05. The input, the weight, and the bias are one-dimensional. Additionally, the input can be passed as an argument.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_act</span>(<span class="params">i=<span class="number">1.0</span>, actfunc=<span class="keyword">lambda</span> x: x</span>):</span><br><span class="line">    ws = np.arange(-<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.05</span>)</span><br><span class="line">    bs = np.arange(-<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">    X, Y = np.meshgrid(ws, bs)</span><br><span class="line"></span><br><span class="line">    os = np.array([actfunc(tf.constant(w*i + b)).<span class="built_in">eval</span>(session=sess) \</span><br><span class="line">                   <span class="keyword">for</span> w,b <span class="keyword">in</span> <span class="built_in">zip</span>(np.ravel(X), np.ravel(Y))])</span><br><span class="line"></span><br><span class="line">    Z = os.reshape(X.shape)</span><br><span class="line"></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>, projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">    ax.plot_surface(X, Y, Z, rstride=<span class="number">1</span>, cstride=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Basic Structure</strong></p>
<span id="more"></span>
<p>In this example we illustrate how, in Tensorflow, to compute the weighted sum that goes into the neuron and direct it to the activation function. For further details, read the code comments below.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#start a session</span></span><br><span class="line">sess = tf.Session();</span><br><span class="line"><span class="comment">#create a simple input of 3 real values</span></span><br><span class="line">i = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], shape=[<span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment">#create a matrix of weights</span></span><br><span class="line">w = tf.random_normal(shape=[<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment">#create a vector of biases</span></span><br><span class="line">b = tf.random_normal(shape=[<span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment">#dummy activation function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">x</span>): <span class="keyword">return</span> x</span><br><span class="line"><span class="comment">#tf.matmul will multiply the input(i) tensor and the weight(w) tensor then sum the result with the bias(b) tensor.</span></span><br><span class="line">act = func(tf.matmul(i, w) + b)</span><br><span class="line"><span class="comment">#Evaluate the tensor to a numpy array</span></span><br><span class="line">act.<span class="built_in">eval</span>(session=sess)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_act(<span class="number">1.0</span>, func)</span><br></pre></td></tr></table></figure>
<h2 id="The-Step-Functions"><a href="#The-Step-Functions" class="headerlink" title="The Step Functions"></a>The Step Functions</h2><p>The Step function was the first one designed for Machine Learning algorithms. It consists of a simple threshold function that varies the Y value from 0 to 1. This function has been historically utilized for classification problems, like Logistic Regression with two classes.</p>
<p><img src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/blog/tensorflow-learn/step_function.jpg" alt=""></p>
<p>The Step Function simply functions as a limiter. Every input that goes through this function will be applied to gets either assigned a value of 0 or 1. As such, it is easy to see how it can be handy in classification problems.</p>
<p>There are other variations of the Step Function such as the Rectangle Step and others, but those are seldom used.</p>
<p>Tensorflow dosenâ€™t have a Step Function.</p>
<h2 id="The-Sigmoid-Functions"><a href="#The-Sigmoid-Functions" class="headerlink" title="The Sigmoid Functions"></a>The Sigmoid Functions</h2><p>The next in line for Machine Learning problems is the family of the ever-present Sigmoid functions. Sigmoid functions are called that due to their shape in the Cartesian plane, which resembles an â€œSâ€ shape.</p>
<p>Sigmoid functions are very useful in the sense that they â€œsquashâ€ their given inputs into a bounded interval. This is exceptionally handy when combining these functions with others such as the Step function.</p>
<p>Most of the Sigmoid functions you should find in applications will be the Logistic, Arctangent, and Hyperbolic Tangent functions.</p>
<h3 id="Logistic-Regression-sigmoid"><a href="#Logistic-Regression-sigmoid" class="headerlink" title="Logistic Regression (sigmoid)"></a>Logistic Regression (sigmoid)</h3><p>The Logistic function, as its name implies, is widely used in Logistic Regression. It is defined as $f(x) = \dfrac{1}{1 + e^{-x}}$. Effectively, this makes it so you have a Sigmoid over the $(0,1)$ interval, like so:</p>
<p><img src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/blog/tensorflow-learn/sigmoid_function.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_act(<span class="number">1</span>, tf.sigmoid)</span><br></pre></td></tr></table></figure>
<p>3D sigmoid plot. The x-axis is the weight, the y-axis is the bias.</p>
<h4 id="Using-sigmoid-in-a-neural-net-layer"><a href="#Using-sigmoid-in-a-neural-net-layer" class="headerlink" title="Using sigmoid in a neural net layer"></a>Using sigmoid in a neural net layer</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">act = tf.sigmoid(tf.matmul(i, w) + b)</span><br><span class="line">act.<span class="built_in">eval</span>(session=sess)</span><br></pre></td></tr></table></figure>
<p>The Arctangent and Hyperbolic Tangent functions on the other hand, as the name implies, are based on the Tangent function. Arctangent is defined by $f(x) = tan^{-1}x$, and produces a sigmoid over the $(\dfrac{-\pi}{2},\dfrac{\pi}{2})$ interval.</p>
<p><img src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/blog/tensorflow-learn/tanh_function.jpg" alt=""></p>
<p>It has no implementation in Tensorflow</p>
<h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p>The Hyperbolic Tangent, or TanH as itâ€™s usually called, is defined as $f(x) = \dfrac{2}{1 + e^{-2x}} - 1$. It produces a sigmoid over the $(-1,1)$ interval. TanH is widely used in a wide range of applications, and is probably the most used function of the Sigmoid family.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_act(<span class="number">1</span>, tf.tanh)</span><br></pre></td></tr></table></figure>
<p>3D tanh plot. The x-axis is the weight, the y-axis is the bias.</p>
<h4 id="Using-tanh-in-a-neural-net-layer"><a href="#Using-tanh-in-a-neural-net-layer" class="headerlink" title="Using tanh in a neural net layer"></a>Using tanh in a neural net layer</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">act = tf.tanh(tf.matmul(i, w) + b)</span><br><span class="line">act.<span class="built_in">eval</span>(session=sess)</span><br></pre></td></tr></table></figure>
<h2 id="The-Linear-Unit-functions"><a href="#The-Linear-Unit-functions" class="headerlink" title="The Linear Unit functions"></a>The Linear Unit functions</h2><p>Linear Units are the next step in activation functions. They take concepts from both Step and Sigmoid functions and behave within the best of the two types of functions. Linear Units in general tend to be variation of what is called the Rectified Linear Unit, or ReLU for short.</p>
<p>The ReLU is a simple function which operates within the $[0,\infty)$ interval. For the entirety of the negative value domain, it returns a value of 0, while on the positive value domain, it returns $x$ for any $f(x)$.</p>
<p><img src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/blog/tensorflow-learn/RELU_function.jpg" alt=""></p>
<p>While it may seem counterintuitive to utilize a pseudo-linear function instead of something like Sigmoids, ReLUs provide some benefits which might not be understood at first glance. For example, during the initialization process of a Neural Network model, in which weights are distributed at random for each unit, ReLUs will only activate approximately only in 50% of the times â€” which saves some processing power. Additionally, the ReLU structure takes care of what is called the <strong>Vanishing and Exploding Gradient</strong> problem by itself. Another benefit â€” if not only marginally relevant to us â€” is that this kind of activation function is directly relatable to the nervous system analogy of Neural Networks (this is called <em>Biological Plausibility</em>).</p>
<p>The ReLU structure has also has many variations optimized for certain applications, but those are implemented on a case-by-case basis and therefore arenâ€™t in the scope of this notebook. If you want to know more, search for <em>Parametric Rectified Linear Units</em> or maybe <em>Exponential Linear Units</em>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_act(<span class="number">1</span>, tf.nn.relu)</span><br></pre></td></tr></table></figure>
<p>3D relu plot. The x-axis is the weight, the y-axis is the bias.</p>
<h3 id="Using-relu-in-a-neural-net-layer"><a href="#Using-relu-in-a-neural-net-layer" class="headerlink" title="Using relu in a neural net layer"></a>Using relu in a neural net layer</h3><p>TensorFlow has ReLU and some other variants of this function. Take a look:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">act = tf.nn.relu(tf.matmul(i, w) + b)</span><br><span class="line">act.<span class="built_in">eval</span>(session=sess)</span><br></pre></td></tr></table></figure>
<p>This is the end of the <strong>Activation Functions</strong> notebook. Hopefully, now you have a deeper understanding of what activation functions are and what they are used for. Thank you for reading this notebook, and good luck on your studies.</p>
<p> You can take a look at all TensorFlow Activation Functions in <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_guides/python/nn#activation-functions">its reference</a>.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">æ–‡ç« ä½œè€…: </span><span class="post-copyright-info"><a href="https://busyboxs.github.io">Busyboxs</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">æ–‡ç« é“¾æ¥: </span><span class="post-copyright-info"><a href="https://busyboxs.github.io/blogs/2b14ee1d/">https://busyboxs.github.io/blogs/2b14ee1d/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">ç‰ˆæƒå£°æ˜: </span><span class="post-copyright-info">æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥è‡ª <a href="https://busyboxs.github.io" target="_blank">Busyboxs</a>ï¼</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/tensorflow/">tensorflow</a></div><div class="post_share"></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/blogs/233b154a/"><img class="prev-cover" src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/covers/tensorflow.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">ä¸Šä¸€ç¯‡</div><div class="prev_info">tensorflowå­¦ä¹ ç¬”è®°ï¼ˆ3ï¼‰ï¼šLOGISTIC REGRESSION WITH TENSORFLOW</div></div></a></div><div class="next-post pull-right"><a href="/blogs/29d4c18e/"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/covers/chrome.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">ä¸‹ä¸€ç¯‡</div><div class="next_info">chromeæµè§ˆå™¨å¤–é“¾å¼¹å‡ºé¡µé¢ä¸ºç©ºç™½é¡µè§£å†³æ–¹æ³•ï¼Ÿ</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>ç›¸å…³æ¨è</span></div><div class="relatedPosts-list"><div><a href="/blogs/7bde1198/" title="ã€è¯‘ã€‘Tensorflow ä¸ Kerasï¼Ÿé€šè¿‡æ„å»ºå›¾åƒåˆ†ç±»æ¨¡å‹æ¥æ¯”è¾ƒ"><img class="cover" src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/covers/tensorflow.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-09-14</div><div class="title">ã€è¯‘ã€‘Tensorflow ä¸ Kerasï¼Ÿé€šè¿‡æ„å»ºå›¾åƒåˆ†ç±»æ¨¡å‹æ¥æ¯”è¾ƒ</div></div></a></div><div><a href="/blogs/f77e32b8/" title="TFRecord æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•ä½¿ç”¨?"><img class="cover" src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/covers/tensorflow.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-08-26</div><div class="title">TFRecord æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•ä½¿ç”¨?</div></div></a></div><div><a href="/blogs/fe533d02/" title="tensorflowå­¦ä¹ ç¬”è®°ï¼ˆ1ï¼‰ï¼šTENSORFLOW&#39;S HELLO WORLD"><img class="cover" src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/covers/tensorflow.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2017-08-11</div><div class="title">tensorflowå­¦ä¹ ç¬”è®°ï¼ˆ1ï¼‰ï¼šTENSORFLOW&#39;S HELLO WORLD</div></div></a></div><div><a href="/blogs/233b154a/" title="tensorflowå­¦ä¹ ç¬”è®°ï¼ˆ3ï¼‰ï¼šLOGISTIC REGRESSION WITH TENSORFLOW"><img class="cover" src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/covers/tensorflow.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2017-08-27</div><div class="title">tensorflowå­¦ä¹ ç¬”è®°ï¼ˆ3ï¼‰ï¼šLOGISTIC REGRESSION WITH TENSORFLOW</div></div></a></div><div><a href="/blogs/94e07831/" title="tensorflowå­¦ä¹ ç¬”è®°ï¼ˆ2ï¼‰ï¼šLINEAR REGRESSION WITH TENSORFLOW"><img class="cover" src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/covers/tensorflow.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2017-08-27</div><div class="title">tensorflowå­¦ä¹ ç¬”è®°ï¼ˆ2ï¼‰ï¼šLINEAR REGRESSION WITH TENSORFLOW</div></div></a></div><div><a href="/blogs/97eeec88/" title="é€šè¿‡åˆ†æå®ä¾‹æ¥ç†è§£TFRecordæ•°æ®çš„è¯»å–ä¸å†™å…¥"><img class="cover" src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/covers/tensorflow.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-09-04</div><div class="title">é€šè¿‡åˆ†æå®ä¾‹æ¥ç†è§£TFRecordæ•°æ®çš„è¯»å–ä¸å†™å…¥</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>ç›®å½•</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Step-Functions"><span class="toc-number">1.</span> <span class="toc-text">The Step Functions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Sigmoid-Functions"><span class="toc-number">2.</span> <span class="toc-text">The Sigmoid Functions</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Logistic-Regression-sigmoid"><span class="toc-number">2.1.</span> <span class="toc-text">Logistic Regression (sigmoid)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Using-sigmoid-in-a-neural-net-layer"><span class="toc-number">2.1.1.</span> <span class="toc-text">Using sigmoid in a neural net layer</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tanh"><span class="toc-number">2.2.</span> <span class="toc-text">Tanh</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Using-tanh-in-a-neural-net-layer"><span class="toc-number">2.2.1.</span> <span class="toc-text">Using tanh in a neural net layer</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Linear-Unit-functions"><span class="toc-number">3.</span> <span class="toc-text">The Linear Unit functions</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Using-relu-in-a-neural-net-layer"><span class="toc-number">3.1.</span> <span class="toc-text">Using relu in a neural net layer</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/img/archive_img.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2022 By Busyboxs</div><div class="framework-info"><span>æ¡†æ¶ </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>ä¸»é¢˜ </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">â¤ğŸ¦‹Stay foolish. Stay hungry.ğŸ¦‹â¤</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="é˜…è¯»æ¨¡å¼"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="æµ…è‰²å’Œæ·±è‰²æ¨¡å¼è½¬æ¢"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="å•æ å’ŒåŒæ åˆ‡æ¢"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="è®¾ç½®"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="ç›®å½•"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="å›åˆ°é¡¶éƒ¨"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script></div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-show-text.min.js" data-mobile="false" data-text="çˆ±ä½ å“Ÿ,å¥½å¥½å­¦ä¹ ï¼Œå¤©å¤©å‘ä¸Š,Stay hungry. Stay foolish.,âœ¨âœ¨âœ¨,ğŸˆğŸˆğŸˆ,ğŸ®ğŸ®,â™¥â™¥â™¥â™¥,ğŸğŸ,ğŸª‚ğŸª‚" data-fontsize="10px" data-random="true" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>