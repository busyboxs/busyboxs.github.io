<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>tensorflow学习笔记（4）：Activation Functions | Busyboxs</title><meta name="keywords" content="tensorflow"><meta name="author" content="Busyboxs"><meta name="copyright" content="Busyboxs"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Activation functions are a cornerstone of Machine Learning. In general, Activation Functions define how a processing unit will treat its input — usually passing this input through it and generating an">
<meta property="og:type" content="article">
<meta property="og:title" content="tensorflow学习笔记（4）：Activation Functions">
<meta property="og:url" content="https://busyboxs.github.io/blogs/2b14ee1d/index.html">
<meta property="og:site_name" content="Busyboxs">
<meta property="og:description" content="Activation functions are a cornerstone of Machine Learning. In general, Activation Functions define how a processing unit will treat its input — usually passing this input through it and generating an">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/covers/tensorflow.jpg">
<meta property="article:published_time" content="2017-08-27T13:00:00.000Z">
<meta property="article:modified_time" content="2019-08-13T02:43:49.627Z">
<meta property="article:author" content="Busyboxs">
<meta property="article:tag" content="tensorflow">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/covers/tensorflow.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://busyboxs.github.io/blogs/2b14ee1d/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google_site_verification" content="FwkuU16DW0uUtLvBlI63aK9-eCBhC55pVQMscORY34M"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?0737f86b9c26e7c4269a4dec38cea0c8";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=UA-158970947-1"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-158970947-1');
</script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lacquer&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'tensorflow学习笔记（4）：Activation Functions',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2019-08-13 10:43:49'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">90</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/img/archive_img.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Busyboxs</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">tensorflow学习笔记（4）：Activation Functions</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2017-08-27T13:00:00.000Z" title="发表于 2017-08-27 21:00:00">2017-08-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2019-08-13T02:43:49.627Z" title="更新于 2019-08-13 10:43:49">2019-08-13</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/tensorflow/">tensorflow</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">1.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>7分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="tensorflow学习笔记（4）：Activation Functions"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>Activation functions are a cornerstone of Machine Learning. In general, Activation Functions define how a processing unit will treat its input — usually passing this input through it and generating an output through its result. To begin the process of having a more intuitive understanding, let’s go through some of the most commonly used functions.</p>
<p><strong>Importing Dependencies</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<p>The next cell implements a basic function that plots a surface for an arbitrary activation function. The plot is done for all possible values of weight and bias between -0.5 and 0.5 with a step of 0.05. The input, the weight, and the bias are one-dimensional. Additionally, the input can be passed as an argument.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_act</span>(<span class="params">i=<span class="number">1.0</span>, actfunc=<span class="keyword">lambda</span> x: x</span>):</span><br><span class="line">    ws = np.arange(-<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.05</span>)</span><br><span class="line">    bs = np.arange(-<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">    X, Y = np.meshgrid(ws, bs)</span><br><span class="line"></span><br><span class="line">    os = np.array([actfunc(tf.constant(w*i + b)).<span class="built_in">eval</span>(session=sess) \</span><br><span class="line">                   <span class="keyword">for</span> w,b <span class="keyword">in</span> <span class="built_in">zip</span>(np.ravel(X), np.ravel(Y))])</span><br><span class="line"></span><br><span class="line">    Z = os.reshape(X.shape)</span><br><span class="line"></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>, projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">    ax.plot_surface(X, Y, Z, rstride=<span class="number">1</span>, cstride=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Basic Structure</strong></p>
<span id="more"></span>
<p>In this example we illustrate how, in Tensorflow, to compute the weighted sum that goes into the neuron and direct it to the activation function. For further details, read the code comments below.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#start a session</span></span><br><span class="line">sess = tf.Session();</span><br><span class="line"><span class="comment">#create a simple input of 3 real values</span></span><br><span class="line">i = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], shape=[<span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment">#create a matrix of weights</span></span><br><span class="line">w = tf.random_normal(shape=[<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment">#create a vector of biases</span></span><br><span class="line">b = tf.random_normal(shape=[<span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment">#dummy activation function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">x</span>): <span class="keyword">return</span> x</span><br><span class="line"><span class="comment">#tf.matmul will multiply the input(i) tensor and the weight(w) tensor then sum the result with the bias(b) tensor.</span></span><br><span class="line">act = func(tf.matmul(i, w) + b)</span><br><span class="line"><span class="comment">#Evaluate the tensor to a numpy array</span></span><br><span class="line">act.<span class="built_in">eval</span>(session=sess)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_act(<span class="number">1.0</span>, func)</span><br></pre></td></tr></table></figure>
<h2 id="The-Step-Functions"><a href="#The-Step-Functions" class="headerlink" title="The Step Functions"></a>The Step Functions</h2><p>The Step function was the first one designed for Machine Learning algorithms. It consists of a simple threshold function that varies the Y value from 0 to 1. This function has been historically utilized for classification problems, like Logistic Regression with two classes.</p>
<p><img src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/blog/tensorflow-learn/step_function.jpg" alt=""></p>
<p>The Step Function simply functions as a limiter. Every input that goes through this function will be applied to gets either assigned a value of 0 or 1. As such, it is easy to see how it can be handy in classification problems.</p>
<p>There are other variations of the Step Function such as the Rectangle Step and others, but those are seldom used.</p>
<p>Tensorflow dosen’t have a Step Function.</p>
<h2 id="The-Sigmoid-Functions"><a href="#The-Sigmoid-Functions" class="headerlink" title="The Sigmoid Functions"></a>The Sigmoid Functions</h2><p>The next in line for Machine Learning problems is the family of the ever-present Sigmoid functions. Sigmoid functions are called that due to their shape in the Cartesian plane, which resembles an “S” shape.</p>
<p>Sigmoid functions are very useful in the sense that they “squash” their given inputs into a bounded interval. This is exceptionally handy when combining these functions with others such as the Step function.</p>
<p>Most of the Sigmoid functions you should find in applications will be the Logistic, Arctangent, and Hyperbolic Tangent functions.</p>
<h3 id="Logistic-Regression-sigmoid"><a href="#Logistic-Regression-sigmoid" class="headerlink" title="Logistic Regression (sigmoid)"></a>Logistic Regression (sigmoid)</h3><p>The Logistic function, as its name implies, is widely used in Logistic Regression. It is defined as $f(x) = \dfrac{1}{1 + e^{-x}}$. Effectively, this makes it so you have a Sigmoid over the $(0,1)$ interval, like so:</p>
<p><img src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/blog/tensorflow-learn/sigmoid_function.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_act(<span class="number">1</span>, tf.sigmoid)</span><br></pre></td></tr></table></figure>
<p>3D sigmoid plot. The x-axis is the weight, the y-axis is the bias.</p>
<h4 id="Using-sigmoid-in-a-neural-net-layer"><a href="#Using-sigmoid-in-a-neural-net-layer" class="headerlink" title="Using sigmoid in a neural net layer"></a>Using sigmoid in a neural net layer</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">act = tf.sigmoid(tf.matmul(i, w) + b)</span><br><span class="line">act.<span class="built_in">eval</span>(session=sess)</span><br></pre></td></tr></table></figure>
<p>The Arctangent and Hyperbolic Tangent functions on the other hand, as the name implies, are based on the Tangent function. Arctangent is defined by $f(x) = tan^{-1}x$, and produces a sigmoid over the $(\dfrac{-\pi}{2},\dfrac{\pi}{2})$ interval.</p>
<p><img src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/blog/tensorflow-learn/tanh_function.jpg" alt=""></p>
<p>It has no implementation in Tensorflow</p>
<h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p>The Hyperbolic Tangent, or TanH as it’s usually called, is defined as $f(x) = \dfrac{2}{1 + e^{-2x}} - 1$. It produces a sigmoid over the $(-1,1)$ interval. TanH is widely used in a wide range of applications, and is probably the most used function of the Sigmoid family.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_act(<span class="number">1</span>, tf.tanh)</span><br></pre></td></tr></table></figure>
<p>3D tanh plot. The x-axis is the weight, the y-axis is the bias.</p>
<h4 id="Using-tanh-in-a-neural-net-layer"><a href="#Using-tanh-in-a-neural-net-layer" class="headerlink" title="Using tanh in a neural net layer"></a>Using tanh in a neural net layer</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">act = tf.tanh(tf.matmul(i, w) + b)</span><br><span class="line">act.<span class="built_in">eval</span>(session=sess)</span><br></pre></td></tr></table></figure>
<h2 id="The-Linear-Unit-functions"><a href="#The-Linear-Unit-functions" class="headerlink" title="The Linear Unit functions"></a>The Linear Unit functions</h2><p>Linear Units are the next step in activation functions. They take concepts from both Step and Sigmoid functions and behave within the best of the two types of functions. Linear Units in general tend to be variation of what is called the Rectified Linear Unit, or ReLU for short.</p>
<p>The ReLU is a simple function which operates within the $[0,\infty)$ interval. For the entirety of the negative value domain, it returns a value of 0, while on the positive value domain, it returns $x$ for any $f(x)$.</p>
<p><img src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/blog/tensorflow-learn/RELU_function.jpg" alt=""></p>
<p>While it may seem counterintuitive to utilize a pseudo-linear function instead of something like Sigmoids, ReLUs provide some benefits which might not be understood at first glance. For example, during the initialization process of a Neural Network model, in which weights are distributed at random for each unit, ReLUs will only activate approximately only in 50% of the times — which saves some processing power. Additionally, the ReLU structure takes care of what is called the <strong>Vanishing and Exploding Gradient</strong> problem by itself. Another benefit — if not only marginally relevant to us — is that this kind of activation function is directly relatable to the nervous system analogy of Neural Networks (this is called <em>Biological Plausibility</em>).</p>
<p>The ReLU structure has also has many variations optimized for certain applications, but those are implemented on a case-by-case basis and therefore aren’t in the scope of this notebook. If you want to know more, search for <em>Parametric Rectified Linear Units</em> or maybe <em>Exponential Linear Units</em>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_act(<span class="number">1</span>, tf.nn.relu)</span><br></pre></td></tr></table></figure>
<p>3D relu plot. The x-axis is the weight, the y-axis is the bias.</p>
<h3 id="Using-relu-in-a-neural-net-layer"><a href="#Using-relu-in-a-neural-net-layer" class="headerlink" title="Using relu in a neural net layer"></a>Using relu in a neural net layer</h3><p>TensorFlow has ReLU and some other variants of this function. Take a look:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">act = tf.nn.relu(tf.matmul(i, w) + b)</span><br><span class="line">act.<span class="built_in">eval</span>(session=sess)</span><br></pre></td></tr></table></figure>
<p>This is the end of the <strong>Activation Functions</strong> notebook. Hopefully, now you have a deeper understanding of what activation functions are and what they are used for. Thank you for reading this notebook, and good luck on your studies.</p>
<p> You can take a look at all TensorFlow Activation Functions in <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_guides/python/nn#activation-functions">its reference</a>.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://busyboxs.github.io">Busyboxs</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://busyboxs.github.io/blogs/2b14ee1d/">https://busyboxs.github.io/blogs/2b14ee1d/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://busyboxs.github.io" target="_blank">Busyboxs</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/tensorflow/">tensorflow</a></div><div class="post_share"></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/blogs/233b154a/"><img class="prev-cover" src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/covers/tensorflow.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">tensorflow学习笔记（3）：LOGISTIC REGRESSION WITH TENSORFLOW</div></div></a></div><div class="next-post pull-right"><a href="/blogs/29d4c18e/"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/covers/chrome.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">chrome浏览器外链弹出页面为空白页解决方法？</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/blogs/7bde1198/" title="【译】Tensorflow 与 Keras？通过构建图像分类模型来比较"><img class="cover" src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/covers/tensorflow.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-09-14</div><div class="title">【译】Tensorflow 与 Keras？通过构建图像分类模型来比较</div></div></a></div><div><a href="/blogs/f77e32b8/" title="TFRecord 是什么？如何使用?"><img class="cover" src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/covers/tensorflow.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-08-26</div><div class="title">TFRecord 是什么？如何使用?</div></div></a></div><div><a href="/blogs/fe533d02/" title="tensorflow学习笔记（1）：TENSORFLOW&#39;S HELLO WORLD"><img class="cover" src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/covers/tensorflow.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2017-08-11</div><div class="title">tensorflow学习笔记（1）：TENSORFLOW&#39;S HELLO WORLD</div></div></a></div><div><a href="/blogs/233b154a/" title="tensorflow学习笔记（3）：LOGISTIC REGRESSION WITH TENSORFLOW"><img class="cover" src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/covers/tensorflow.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2017-08-27</div><div class="title">tensorflow学习笔记（3）：LOGISTIC REGRESSION WITH TENSORFLOW</div></div></a></div><div><a href="/blogs/94e07831/" title="tensorflow学习笔记（2）：LINEAR REGRESSION WITH TENSORFLOW"><img class="cover" src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/covers/tensorflow.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2017-08-27</div><div class="title">tensorflow学习笔记（2）：LINEAR REGRESSION WITH TENSORFLOW</div></div></a></div><div><a href="/blogs/97eeec88/" title="通过分析实例来理解TFRecord数据的读取与写入"><img class="cover" src="https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/covers/tensorflow.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-09-04</div><div class="title">通过分析实例来理解TFRecord数据的读取与写入</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Step-Functions"><span class="toc-number">1.</span> <span class="toc-text">The Step Functions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Sigmoid-Functions"><span class="toc-number">2.</span> <span class="toc-text">The Sigmoid Functions</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Logistic-Regression-sigmoid"><span class="toc-number">2.1.</span> <span class="toc-text">Logistic Regression (sigmoid)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Using-sigmoid-in-a-neural-net-layer"><span class="toc-number">2.1.1.</span> <span class="toc-text">Using sigmoid in a neural net layer</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tanh"><span class="toc-number">2.2.</span> <span class="toc-text">Tanh</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Using-tanh-in-a-neural-net-layer"><span class="toc-number">2.2.1.</span> <span class="toc-text">Using tanh in a neural net layer</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Linear-Unit-functions"><span class="toc-number">3.</span> <span class="toc-text">The Linear Unit functions</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Using-relu-in-a-neural-net-layer"><span class="toc-number">3.1.</span> <span class="toc-text">Using relu in a neural net layer</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://cdn.jsdelivr.net/gh/busyboxs/CDN@latest/img/archive_img.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2022 By Busyboxs</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">❤🦋Stay foolish. Stay hungry.🦋❤</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script></div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-show-text.min.js" data-mobile="false" data-text="爱你哟,好好学习，天天向上,Stay hungry. Stay foolish.,✨✨✨,🎈🎈🎈,🎮🎮,♥♥♥♥,🏐🏐,🪂🪂" data-fontsize="10px" data-random="true" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>